{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Hybrid Pull Request Summarization with Gemini\n",
    "\n",
    "# This notebook demonstrates:\n",
    "# 1. Loading a small set of GitHub PRs\n",
    "# 2. Cleaning / normalizing diffs\n",
    "# 3. Generating PR descriptions using Gemini API\n",
    "# 4. Evaluating generated descriptions using ROUGE-L"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6575ec96a3407f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\") \n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_colwidth = 400"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24eeac5d83158c67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Load PR dataset\n",
    "df = pd.read_csv(\"../data/pull_requests.csv\")\n",
    "df_small = df.head(5).copy()\n",
    "df_small.head(3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "168918ef5056b0ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Preprocessing\n",
    "import re\n",
    "\n",
    "def trim_diff(diff: str, max_len: int = 4000) -> str:\n",
    "    if not isinstance(diff, str):\n",
    "        return \"\"\n",
    "    return diff[:max_len]\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df_small[\"diff_clean\"] = df_small[\"diff\"].apply(lambda d: normalize_text(trim_diff(d)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f598e5349caaae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Gemini PR summarization\n",
    "from src.llm_client import safe_generate\n",
    "\n",
    "df_small[\"description_generated\"] = \"\"\n",
    "for idx, row in tqdm(df_small.iterrows(), total=len(df_small)):\n",
    "    diff_text = row[\"diff_clean\"]\n",
    "    if diff_text:\n",
    "        df_small.at[idx, \"description_generated\"] = safe_generate(diff_text)\n",
    "\n",
    "# Save results\n",
    "df_small.to_csv(\"../results/notebook_summaries.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "df_small[[\"pr_number\", \"description_original\", \"description_generated\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4b109584370df5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. ROUGE-L evaluation\n",
    "from src.evaluate import compute_rouge_l\n",
    "\n",
    "refs = df_small[\"description_original\"].fillna(\"\").tolist()\n",
    "preds = df_small[\"description_generated\"].fillna(\"\").tolist()\n",
    "\n",
    "rouge_score = compute_rouge_l(refs, preds)\n",
    "print(\"Average ROUGE-L:\", rouge_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4af60d8c069ab0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps\n",
    "# - Run full scripts in terminal for bigger datasets.\n",
    "# - Collect human evaluation using Google Forms and aggregate results.\n",
    "# - Compare ROUGE-L across multiple PRs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5186cad242c29db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
